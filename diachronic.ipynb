{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TreebankWordTokenizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes an html file, converts it to text, and adds it to the corpus list\n",
    "def html_to_txt(h_file):\n",
    "    with open(h_file, \"r\") as file:\n",
    "        html = file.read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    output = soup.get_text()\n",
    "    corpus.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#folder with library files as a variable: data_path\n",
    "\n",
    "dir = Path.cwd()\n",
    "data_path = dir / 'data' / 'a-library-nlp-project/theanarchistlibrary.org' / 'library'\n",
    "data_path.exists()\n",
    "data_path.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all html files, convert them to txt, add them to corpus\n",
    "#this takes 7.5 min to run :O\n",
    "files = data_path.glob('*.html')\n",
    "\n",
    "for file in files:\n",
    "    html_to_txt(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/josh/Documents/a-library-nlp-project/theanarchistlibrary.org/library/zvonimir-kontrec-architecture-is-a-political-act.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#word2vec needs list of lists, so I'm going to open 2 files\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/josh/Documents/a-library-nlp-project/theanarchistlibrary.org/library/zvonimir-kontrec-architecture-is-a-political-act.html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m     html \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/josh/Documents/a-library-nlp-project/theanarchistlibrary.org/library/zvonimir-kontrec-architecture-is-a-political-act.html'"
     ]
    }
   ],
   "source": [
    "#word2vec needs list of lists, so I'm going to open 2 files\n",
    "\n",
    "with open(\"/home/josh/Documents/a-library-nlp-project/theanarchistlibrary.org/library/zvonimir-kontrec-architecture-is-a-political-act.html\", \"r\") as file:\n",
    "    html = file.read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "with open(\"/home/josh/Documents/a-library-nlp-project/theanarchistlibrary.org/library/zundlumpen-will-o-the-wisps.html\", \"r\") as file:\n",
    "    html2 = file.read()\n",
    "soup2 = BeautifulSoup(html2, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert html to text\n",
    "first_small_doc = soup.get_text()\n",
    "second_small_doc = soup2.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_small_doc\n",
    "#print(first_small_doc)\n",
    "#print(second_small_doc)\n",
    "\n",
    "#calling it by itself shows a bunch of line breaks and formatting junk\n",
    "#printing it makes it all nice and pretty and readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "docs.append(first_small_doc)\n",
    "docs.append(second_small_doc)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "docs_tokenized = []\n",
    "for doc in docs:\n",
    "    docs_tokenized.append(tokenizer.tokenize(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build skipgram model\n",
    "model_sg = Word2Vec(sentences=docs_tokenized, window=5, min_count=3, workers=4, epochs=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('for', 0.9980694651603699),\n",
       " ('this', 0.9979365468025208),\n",
       " ('In', 0.9979010820388794),\n",
       " ('system', 0.9978348016738892),\n",
       " ('way', 0.9978195428848267),\n",
       " ('in', 0.9977800846099854),\n",
       " ('their', 0.9977703094482422),\n",
       " ('towards', 0.9977662563323975),\n",
       " ('building', 0.9977622032165527),\n",
       " ('after', 0.9977437257766724)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar('the', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build CBOW model\n",
    "model_cbow = Word2Vec(sentences=docs_tokenized, window=5, min_count=3, workers=4, epochs=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('through', 0.9726642370223999),\n",
       " ('into', 0.9719277024269104),\n",
       " (\"n't\", 0.9718241691589355),\n",
       " ('was', 0.9717423915863037),\n",
       " ('also', 0.9716292023658752),\n",
       " (')', 0.9716008305549622),\n",
       " ('it', 0.971388041973114),\n",
       " ('I', 0.9713448286056519),\n",
       " ('(', 0.9712851047515869),\n",
       " ('is', 0.9712598919868469)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow.wv.most_similar('media', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok, now let's do it for a few more docs and compare cosine cimilarity between two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0.9982710480690002),\n",
       " ('ideas', 0.9982483386993408),\n",
       " ('left', 0.9982380867004395),\n",
       " ('many', 0.9981945157051086),\n",
       " ('by', 0.9981881380081177),\n",
       " ('to', 0.9981729984283447),\n",
       " (',', 0.9981165528297424),\n",
       " ('as', 0.998098611831665),\n",
       " ('on', 0.9980598092079163),\n",
       " ('radical', 0.9980372786521912)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"/home/josh/Documents/a-library-nlp-project/theanarchistlibrary.org/library/zundlumpen-radical-left-i-m-breaking-up-with-you.html\", \"r\") as file:\n",
    "    html3 = file.read()\n",
    "soup3 = BeautifulSoup(html3, \"html.parser\")\n",
    "with open(\"/home/josh/Documents/a-library-nlp-project/theanarchistlibrary.org/library/zosia-brom-insert-topic-of-the-day-has-divided-anarchists.html\", \"r\") as file:\n",
    "    html4 = file.read()\n",
    "soup4 = BeautifulSoup(html4, \"html.parser\")\n",
    "\n",
    "#convert html to text\n",
    "third_small_doc = soup3.get_text()\n",
    "fourth_small_doc = soup4.get_text()\n",
    "\n",
    "#this could be decade folders when i build this out\n",
    "docs_next_corpus = []\n",
    "docs_next_corpus.append(third_small_doc)\n",
    "docs_next_corpus.append(fourth_small_doc)\n",
    "len(docs_next_corpus)\n",
    "\n",
    "docs_next_tokenized = []\n",
    "for doc in docs_next_corpus:\n",
    "    docs_next_tokenized.append(tokenizer.tokenize(doc))\n",
    "\n",
    "model_sg_next = Word2Vec(sentences=docs_next_tokenized, window=5, min_count=3, workers=4, epochs=5, sg=1)\n",
    "model_sg_next.wv.most_similar('the', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.9978370070457458),\n",
       " ('many', 0.9977409243583679),\n",
       " ('a', 0.997643232345581),\n",
       " ('that', 0.9976242780685425),\n",
       " ('but', 0.9976202845573425),\n",
       " ('we', 0.997607409954071),\n",
       " ('or', 0.9975824356079102),\n",
       " ('by', 0.9975618124008179),\n",
       " ('communist', 0.9975566864013672),\n",
       " ('from', 0.9975111484527588)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding a word that is in both doc lists to see top 10 most similar words\n",
    "model_sg_next.wv.most_similar('political', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('backwards', 0.9973574280738831),\n",
       " ('anti-civilizational', 0.9973565936088562),\n",
       " ('thought', 0.9972497224807739),\n",
       " ('were', 0.9972323179244995),\n",
       " ('life', 0.9972214698791504),\n",
       " ('In', 0.9971413016319275),\n",
       " ('Instead', 0.997123122215271),\n",
       " ('within', 0.9971001148223877),\n",
       " ('so', 0.9970480799674988),\n",
       " ('it', 0.9970154166221619)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar('political', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vectors for the same words in different corpora\n",
    "vector_pol_1 = model_sg.wv['political']\n",
    "vector_pol_2 = model_sg_next.wv['political']\n",
    "\n",
    "vector_the_1 = model_sg.wv['the']\n",
    "vector_the_2 = model_sg_next.wv['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check structure of vector\n",
    "#vector_pol_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(vector):\n",
    "    res = vector.reshape(1, -1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_pol_1 = reshape(vector_pol_1)\n",
    "vector_pol_2 = reshape(vector_pol_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7454266]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vector_pol_1, vector_pol_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_the_1 = reshape(vector_the_1)\n",
    "vector_the_2 = reshape(vector_the_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7461838]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vector_the_1, vector_the_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9966311]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vector_the_1, vector_pol_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99663097"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.similarity(\"the\", \"political\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
